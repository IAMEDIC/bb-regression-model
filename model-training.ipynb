{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4712225b",
   "metadata": {},
   "source": [
    "# Bounding Box Regression Model for Fetal Anatomic Structures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439c53cb",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c3133a",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "id": "342df543",
   "metadata": {},
   "source": [
    "import os\n",
    "import gc\n",
    "from collections import defaultdict\n",
    "from typing import Optional, Iterator\n",
    "import hashlib\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "\n",
    "import torchvision.models as models\n",
    "from torchvision.models.mobilenetv2 import MobileNet_V2_Weights\n",
    "\n",
    "import albumentations as A\n",
    "\n",
    "import numpy as np\n",
    "from numpy.typing import NDArray\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import mlflow\n",
    "from mlflow.data.dataset import Dataset as MLFLowDataset\n",
    "\n",
    "from dotenv import load_dotenv\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cf3e1aaa",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "id": "e1edc686",
   "metadata": {},
   "source": [
    "load_dotenv()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2f51a8c7",
   "metadata": {},
   "source": [
    "%matplotlib tk"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a33b5339",
   "metadata": {},
   "source": [
    "np.random.seed = 0"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4a18b5ee",
   "metadata": {},
   "source": [
    "# Model Configuraton\n",
    "TARGET_HEIGHT = 400\n",
    "TARGET_WIDTH = 600\n",
    "# Dataset Configuration\n",
    "DATASET_DIR = os.getenv(\"DATASET_DIR\")\n",
    "DATASET_IMAGES_DIR_NAME = \"Images\"\n",
    "DATASET_IMAGES_DIR = os.path.join(*[DATASET_DIR, DATASET_IMAGES_DIR_NAME])\n",
    "ANNOTATIONS_PATH = os.path.join(*[DATASET_DIR, \"ObjectDetection.xlsx\"])\n",
    "VERSIONING_FILE_NAME = \"Versioning.xlsx\"\n",
    "VERSIONING_PATH = os.path.join(*[DATASET_DIR, VERSIONING_FILE_NAME])\n",
    "DATASET_VERSION = \"V1\"\n",
    "DATASET_TRAIN_VAL_COL = \"Train + Val Filenames\"\n",
    "DATASET_TEST_COL = \"Test Filenames\"\n",
    "# Experiment logging\n",
    "MLFLOW_URI = os.getenv(\"MLFLOW_URI\")\n",
    "MLFLOW_EXPERIMENT_NAME = f\"Fetal_Structures_BB_Regression_{TARGET_HEIGHT}x{TARGET_WIDTH}\"\n",
    "MLFLOW_USER = os.getenv(\"MLFLOW_USER\")\n",
    "MODEL_NAME = f\"fetal_structures_bb_regressor_{TARGET_HEIGHT}x{TARGET_WIDTH}\"\n",
    "TB_LOG_DIR = r\"./tb_logs\"\n",
    "# Training parameters\n",
    "TRAIN_SPLIT, VAL_SPLIT = 0.8, 0.2\n",
    "K_FOLDS = 1\n",
    "BATCH_SIZE = 8\n",
    "NUM_EPOCHS = 100\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "EARLY_STOPPING_PATIENCE = 10\n",
    "# Temp dirs\n",
    "CHECKPOINTS_DIR = r\"./checkpoints\"\n",
    "if not os.path.exists(CHECKPOINTS_DIR):\n",
    "    os.makedirs(CHECKPOINTS_DIR)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "365e9d20",
   "metadata": {},
   "source": [
    "### Experiment Setup"
   ]
  },
  {
   "cell_type": "code",
   "id": "f6524231",
   "metadata": {},
   "source": [
    "# Logging with tensorboard and mlflow\n",
    "if not os.path.exists(TB_LOG_DIR):\n",
    "    os.makedirs(TB_LOG_DIR, exist_ok=True)\n",
    "TB_WRITER = SummaryWriter(log_dir=TB_LOG_DIR)\n",
    "if MLFLOW_URI is None:\n",
    "    raise RuntimeError(\"MLFLOW_URI environment variable is not set.\")\n",
    "mlflow.set_tracking_uri(MLFLOW_URI)\n",
    "MLFLOW_EXPERIMENT = mlflow.set_experiment(MLFLOW_EXPERIMENT_NAME)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1cd58d57",
   "metadata": {},
   "source": [
    "# Setup device\n",
    "DEVICE = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"\n",
    "# check if windows is being used and try to import torch-directml\n",
    "elif os.name == 'nt':\n",
    "    try:\n",
    "        import torch_directml\n",
    "        DEVICE = torch_directml.device()\n",
    "    except ImportError:\n",
    "        # type: ignore\n",
    "        try:\n",
    "            ! pip install torch-directml==0.2.4.dev240913\n",
    "            import torch_directml\n",
    "            DEVICE = torch_directml.device()\n",
    "        except Exception as e: # pylint: disable=broad-except\n",
    "            raise e\n",
    "    except Exception as e: # pylint: disable=broad-except\n",
    "        print(f\"Error occurred while setting up DirectML: {e}\")\n",
    "print(f\"Using device: {DEVICE}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "67c70be8",
   "metadata": {},
   "source": [
    "## Dataset Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "id": "3564ba46",
   "metadata": {},
   "source": [
    "df = pd.read_excel(ANNOTATIONS_PATH)\n",
    "ANNOTATIONS: dict[str, list[tuple[int, tuple[int, int, int, int]]]] = defaultdict(list)\n",
    "\n",
    "STRUCTURES: list[str] = sorted(set(df['structure']))  # or predefined list\n",
    "STRUCTURE_TO_ID: dict[str, int] = {name: i for i, name in enumerate(STRUCTURES)}\n",
    "ID_TO_STRUCTURE: dict[int, str] = {v: k for k, v in STRUCTURE_TO_ID.items()}\n",
    "NUMBER_OF_CLASSES = len(STRUCTURE_TO_ID)  # should be 9\n",
    "\n",
    "COLORS = plt.get_cmap('tab10', NUMBER_OF_CLASSES)\n",
    "COLORS_MAP = {structure: COLORS(i) for i, structure in enumerate(STRUCTURES)}\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    fname = row['fname']\n",
    "    class_id = STRUCTURE_TO_ID[row['structure']]\n",
    "    \n",
    "    hmin, wmin, hmax, wmax = row['h_min'], row['w_min'], row['h_max'], row['w_max']\n",
    "    box = (wmin, hmin, wmax, hmax)  # (xmin, ymin, xmax, ymax)\n",
    "    ANNOTATIONS[fname].append((class_id, box))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c955a109",
   "metadata": {},
   "source": [
    "print(STRUCTURE_TO_ID)\n",
    "print(ID_TO_STRUCTURE)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ed6a68a2",
   "metadata": {},
   "source": [
    "TRAIN_TRANSFORMS = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.3)\n",
    "])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a35945e8",
   "metadata": {},
   "source": [
    "class AnatomyDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_dir: str,\n",
    "        img_names: Optional[list[str]] = None,\n",
    "    ):\n",
    "        self.img_dir = img_dir\n",
    "        self.img_names = sorted(os.listdir(img_dir)) if img_names is None else img_names\n",
    "        self.n_imgs = len(self.img_names)\n",
    "        self.data: dict[str, NDArray[np.float32]] = {}\n",
    "        self.load_data()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_imgs\n",
    "    \n",
    "    def load_data(self):\n",
    "        self.data = {}\n",
    "        gc.collect()\n",
    "        images = np.zeros((self.n_imgs, TARGET_HEIGHT, TARGET_WIDTH), dtype=np.float32)\n",
    "        dataset_bounding_boxes = np.zeros((self.n_imgs, NUMBER_OF_CLASSES, 4), dtype=np.float32)\n",
    "        dataset_class_masks = np.zeros((self.n_imgs, NUMBER_OF_CLASSES), dtype=np.float32)\n",
    "        for index, fname in enumerate(self.img_names):\n",
    "            img_path = os.path.join(self.img_dir, fname)\n",
    "            image = np.array(Image.open(img_path).convert('L'), dtype=np.float32)\n",
    "            original_height, original_width = image.shape\n",
    "            bboxes_list = []\n",
    "            class_labels_list = []\n",
    "            for class_id, (xmin, ymin, xmax, ymax) in ANNOTATIONS[fname]:\n",
    "                bboxes_list.append([xmin, ymin, xmax, ymax])\n",
    "                class_labels_list.append(class_id)\n",
    "            bounding_boxes = np.zeros((NUMBER_OF_CLASSES, 4), dtype=np.float32)\n",
    "            class_masks = np.zeros(NUMBER_OF_CLASSES, dtype=np.float32)\n",
    "            for class_id, (xmin, ymin, xmax, ymax) in zip(class_labels_list, bboxes_list):\n",
    "                cx = ((xmin + xmax) / 2) / original_width\n",
    "                cy = ((ymin + ymax) / 2) / original_height\n",
    "                bw = (xmax - xmin) / original_width\n",
    "                bh = (ymax - ymin) / original_height\n",
    "                class_masks[int(class_id)] = 1\n",
    "                bounding_boxes[int(class_id)] = np.array([cx, cy, bw, bh], dtype=np.float32)\n",
    "            image = A.Compose([\n",
    "                A.Resize(height=TARGET_HEIGHT, width=TARGET_WIDTH),\n",
    "                A.Normalize(mean=(0.5,), std=(0.5,), max_pixel_value=255.0),\n",
    "            ])(image=image)['image'] # Does not affect yolo format bboxes\n",
    "            images[index] = image\n",
    "            dataset_bounding_boxes[index] = bounding_boxes\n",
    "            dataset_class_masks[index] = class_masks\n",
    "        self.data[\"images\"] = images\n",
    "        self.data[\"bboxes\"] = dataset_bounding_boxes\n",
    "        self.data[\"class_masks\"] = dataset_class_masks\n",
    "    \n",
    "    def get_data(self, idx) -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        return (\n",
    "            self.data[\"images\"][idx],\n",
    "            self.data[\"class_masks\"][idx],\n",
    "            self.data[\"bboxes\"][idx]\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, idx) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        return (\n",
    "            torch.tensor(self.data[\"images\"][idx]).unsqueeze(0),\n",
    "            torch.tensor(self.data[\"class_masks\"][idx]),\n",
    "            torch.tensor(self.data[\"bboxes\"][idx])\n",
    "        )\n",
    "\n",
    "\n",
    "class AnatomyDatasetSubset(AnatomyDataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            base_dataset: AnatomyDataset,\n",
    "            indices: list[int] | NDArray[np.integer],\n",
    "            transform: Optional[A.Compose] = None,\n",
    "        ):\n",
    "        self.base_dataset = base_dataset\n",
    "        self.transform = A.Compose(\n",
    "            [transform if transform is not None else A.NoOp()],\n",
    "            bbox_params=A.BboxParams(\n",
    "                format='yolo',\n",
    "            )\n",
    "        )\n",
    "        self.indices = indices\n",
    "        self.img_names = [self.base_dataset.img_names[i] for i in self.indices]\n",
    "        self.n_imgs = len(self.indices)\n",
    "        self.data: dict[str, NDArray[np.float32]] = {}\n",
    "        self.transform_data()\n",
    "    \n",
    "    def transform_data(self):\n",
    "        self.data = {}\n",
    "        gc.collect()\n",
    "        images = np.zeros((self.n_imgs, TARGET_HEIGHT, TARGET_WIDTH), dtype=np.float32)\n",
    "        dataset_bounding_boxes = np.zeros((self.n_imgs, NUMBER_OF_CLASSES, 4), dtype=np.float32)\n",
    "        dataset_class_masks = np.zeros((self.n_imgs, NUMBER_OF_CLASSES), dtype=np.float32)\n",
    "        for new_index, original_index in enumerate(self.indices):\n",
    "            image, class_masks, bboxes = self.base_dataset.get_data(original_index)\n",
    "            valid_bboxes_indeces = np.where(class_masks == 1)[0]\n",
    "            valid_bboxes = [bboxes[i] for i in valid_bboxes_indeces]\n",
    "            transformed_data = self.transform(\n",
    "                image=image,\n",
    "                bboxes=valid_bboxes)\n",
    "            transformed_image = transformed_data['image']\n",
    "            transformed_valid_bboxes = transformed_data['bboxes']\n",
    "            transformed_bboxes = np.zeros((NUMBER_OF_CLASSES, 4), dtype=np.float32)\n",
    "            for i, bbox in zip(valid_bboxes_indeces, transformed_valid_bboxes):\n",
    "                transformed_bboxes[i] = np.array(bbox, dtype=np.float32)\n",
    "            images[new_index] = transformed_image\n",
    "            dataset_bounding_boxes[new_index] = transformed_bboxes\n",
    "            dataset_class_masks[new_index] = class_masks\n",
    "        self.data[\"images\"] = images\n",
    "        self.data[\"bboxes\"] = dataset_bounding_boxes\n",
    "        self.data[\"class_masks\"] = dataset_class_masks\n",
    "\n",
    "    def __iter__(self) -> Iterator[tuple[torch.Tensor, torch.Tensor, torch.Tensor]]:\n",
    "        for idx in range(len(self)):\n",
    "            yield self[idx]\n",
    "        self.transform_data()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a8cd2ae3",
   "metadata": {},
   "source": [
    "class ImageListDataset(MLFLowDataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            names: list[str],\n",
    "            source: str = VERSIONING_FILE_NAME,\n",
    "            version: str = DATASET_VERSION\n",
    "        ):\n",
    "        self._names = names\n",
    "        self._source = source\n",
    "        self._version = version\n",
    "\n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            \"name\": \"image_list_dataset\",\n",
    "            \"digest\": hashlib.md5(\",\".join(self._names).encode()).hexdigest(),\n",
    "            \"source_type\": \"inline\",\n",
    "            \"source\": self._source,\n",
    "            \"schema\": None,\n",
    "            \"profile\": json.dumps({\n",
    "                \"version\": self._version,\n",
    "                \"num_images\": len(self._names),\n",
    "                \"filenames\": self._names\n",
    "            }),\n",
    "        }"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ba4c332d",
   "metadata": {},
   "source": [
    "VERSIONING_DF = pd.read_excel(VERSIONING_PATH, sheet_name=DATASET_VERSION)\n",
    "TRAIN_VAL_IMG_NAMES = VERSIONING_DF[DATASET_TRAIN_VAL_COL].dropna().tolist()\n",
    "TEST_IMG_NAMES = VERSIONING_DF[DATASET_TEST_COL].dropna().tolist()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2136ae97",
   "metadata": {},
   "source": [
    "if DATASET_DIR is None:\n",
    "    raise RuntimeError(\"DATASET_DIR environment variable is not set.\")\n",
    "TRAIN_VAL_DATASET = AnatomyDataset(\n",
    "    img_dir=DATASET_IMAGES_DIR,\n",
    "    img_names=TRAIN_VAL_IMG_NAMES\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "edc9c738",
   "metadata": {},
   "source": [
    "if DATASET_DIR is None:\n",
    "    raise RuntimeError(\"DATASET_DIR environment variable is not set.\")\n",
    "TEST_DATASET = AnatomyDataset(\n",
    "    img_dir=DATASET_IMAGES_DIR,\n",
    "    img_names=TEST_IMG_NAMES\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d6cb07e7",
   "metadata": {},
   "source": [
    "def visualize_sample(\n",
    "        dataset: AnatomyDataset,\n",
    "        idx: int):\n",
    "    colors = plt.get_cmap('tab10', len(STRUCTURES))\n",
    "    color_map = {structure: colors(i) for i, structure in enumerate(STRUCTURES)}\n",
    "    img, class_mask, boxes = dataset[idx]\n",
    "    img = img.squeeze(0).numpy()  # Convert to 2D array\n",
    "    class_mask = class_mask.numpy()\n",
    "    boxes = boxes.numpy()\n",
    "    img_name = dataset.img_names[idx]\n",
    "    # Convert tensor to numpy for plotting\n",
    "    # Create figure and axes\n",
    "    fig, ax = plt.subplots(1, figsize=(10, 10), num=img_name)\n",
    "    ax.imshow(img, cmap='gray')\n",
    "    # Plot boxes for present classes\n",
    "    present_classes = np.where(class_mask == 1)[0]\n",
    "    for class_idx in present_classes:\n",
    "        structure_name = ID_TO_STRUCTURE[int(class_idx)]\n",
    "        color = color_map[structure_name]\n",
    "        cx, cy, w, h = boxes[class_idx]\n",
    "        # Convert normalized coordinates back to pixel coordinates\n",
    "        cx_px = cx * TARGET_WIDTH\n",
    "        cy_px = cy * TARGET_HEIGHT\n",
    "        w_px = w * TARGET_WIDTH\n",
    "        h_px = h * TARGET_HEIGHT\n",
    "        # Convert center coordinates to top-left coordinates\n",
    "        x = cx_px - w_px/2\n",
    "        y = cy_px - h_px/2\n",
    "        # Create rectangle patch\n",
    "        rect = patches.Rectangle((x, y), w_px, h_px, linewidth=2, \n",
    "                               edgecolor=color, facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        # Get structure name\n",
    "        ax.text(x, y-5, structure_name, color=color_map[structure_name], fontsize=8)\n",
    "    ax.set_title(f'Image: {img_name}')\n",
    "    ax.axis('off')\n",
    "    fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ee5648f7",
   "metadata": {},
   "source": [
    "# sample 3 images from TRAIN_VAL_DATASET\n",
    "indices = np.random.choice(len(TRAIN_VAL_DATASET), size=3, replace=False)\n",
    "subset = AnatomyDatasetSubset(\n",
    "    base_dataset=TRAIN_VAL_DATASET,\n",
    "    indices=indices,\n",
    "    transform=TRAIN_TRANSFORMS\n",
    ")\n",
    "for idx in range(len(subset)):\n",
    "    visualize_sample(subset, idx)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cdc28633",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "id": "3592d4f3",
   "metadata": {},
   "source": [
    "class DetectionHead(nn.Module):\n",
    "    def __init__(self, in_features, num_classes=9):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        # Classifier head: multi-label binary classification\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(in_features, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_classes),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        # Regressor head: 4 coords per class (x, y, w, h)\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(in_features, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_classes * 4)\n",
    "        )\n",
    "\n",
    "    def forward(self, features: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        probs = self.classifier(features)                       # [B, 9]\n",
    "        boxes: torch.Tensor = self.regressor(features)          # [B, 36]\n",
    "        boxes = boxes.view(-1, self.num_classes, 4)             # [B, 9, 4]\n",
    "        return probs, boxes"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "53f2e9e2",
   "metadata": {},
   "source": [
    "class FetusAnatomyBBRegressor(nn.Module):\n",
    "    def __init__(self, num_classes=9):\n",
    "        super().__init__()\n",
    "        # Load pretrained MobileNetV2 and adapt for 1 channel\n",
    "        mobilenet = models.mobilenet_v2(weights=MobileNet_V2_Weights.IMAGENET1K_V1)\n",
    "        self.features = mobilenet.features\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        # Modify first conv layer to accept grayscale\n",
    "        first_conv = self.features[0][0]\n",
    "        new_conv = nn.Conv2d(1, first_conv.out_channels, kernel_size=first_conv.kernel_size,\n",
    "                             stride=first_conv.stride, padding=first_conv.padding, bias=False)\n",
    "        new_conv.weight.data = first_conv.weight.data.mean(dim=1, keepdim=True)\n",
    "        self.features[0][0] = new_conv\n",
    "        # Head\n",
    "        self.head = DetectionHead(in_features=1280, num_classes=num_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        x = self.features(x)                    # [B, 1280, H', W']\n",
    "        x = self.pool(x).view(x.size(0), -1)    # [B, 1280]\n",
    "        return self.head(x)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f63613c2",
   "metadata": {},
   "source": [
    "def detection_loss(\n",
    "        pred_probs: torch.Tensor,\n",
    "        pred_boxes: torch.Tensor,\n",
    "        gt_probs: torch.Tensor,\n",
    "        gt_boxes: torch.Tensor,\n",
    "        lambda_reg: float = 10.0\n",
    "    ) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Combined loss for multi-label classification and bounding box regression.\n",
    "    Args:\n",
    "        pred_probs: [B, K] (sigmoid output)\n",
    "        pred_boxes: [B, K, 4]\n",
    "        gt_probs:   [B, K] (0 or 1)\n",
    "        gt_boxes:   [B, K, 4]\n",
    "    Returns:\n",
    "        loss: scalar tensor\n",
    "    \"\"\"\n",
    "    bce = nn.BCELoss()\n",
    "    cls_loss = bce(pred_probs, gt_probs)\n",
    "    # Compute Smooth L1 per box, then sum over coordinates\n",
    "    smooth_l1 = nn.SmoothL1Loss(reduction='none')\n",
    "    box_loss = smooth_l1(pred_boxes, gt_boxes).sum(dim=-1)  # [B, K]\n",
    "    # Gate with class presence * predicted confidence\n",
    "    gated_loss = gt_probs * pred_probs.detach() * box_loss  # [B, K]\n",
    "    reg_loss = gated_loss.mean()\n",
    "    return cls_loss + lambda_reg * reg_loss"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "29dd5e10",
   "metadata": {},
   "source": [
    "## Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08aa93ad",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "id": "3d5e6ec8",
   "metadata": {},
   "source": [
    "def box_iou(box1: torch.Tensor, box2: torch.Tensor) -> float:\n",
    "    # box1, box2: (cx, cy, w, h) in normalized [0, 1]\n",
    "    box1 = box1.clone()\n",
    "    box2 = box2.clone()\n",
    "    x1_min = float(box1[0] - box1[2]/2)\n",
    "    y1_min = float(box1[1] - box1[3]/2)\n",
    "    x1_max = float(box1[0] + box1[2]/2)\n",
    "    y1_max = float(box1[1] + box1[3]/2)\n",
    "    x2_min = float(box2[0] - box2[2]/2)\n",
    "    y2_min = float(box2[1] - box2[3]/2)\n",
    "    x2_max = float(box2[0] + box2[2]/2)\n",
    "    y2_max = float(box2[1] + box2[3]/2)\n",
    "    inter_xmin = max(x1_min, x2_min)\n",
    "    inter_ymin = max(y1_min, y2_min)\n",
    "    inter_xmax = min(x1_max, x2_max)\n",
    "    inter_ymax = min(y1_max, y2_max)\n",
    "    inter_area = max(0, inter_xmax - inter_xmin) * max(0, inter_ymax - inter_ymin)\n",
    "    area1 = (x1_max - x1_min) * (y1_max - y1_min)\n",
    "    area2 = (x2_max - x2_min) * (y2_max - y2_min)\n",
    "    union = area1 + area2 - inter_area\n",
    "    return inter_area / union if union > 0 else 0.0\n",
    "\n",
    "def compute_map_per_image(\n",
    "    gt_mask: torch.Tensor,\n",
    "    gt_boxes: torch.Tensor,\n",
    "    pred_mask: torch.Tensor,\n",
    "    pred_boxes: torch.Tensor,\n",
    "    iou_thresh: float = 0.5\n",
    ") -> float:\n",
    "    aps = []\n",
    "    for k in range(gt_mask.shape[0]):\n",
    "        gt_present = gt_mask[k].item() == 1\n",
    "        pred_present = pred_mask[k].item() > 0.5\n",
    "        if gt_present and pred_present:\n",
    "            iou = box_iou(gt_boxes[k], pred_boxes[k])\n",
    "            aps.append(1.0 if iou >= iou_thresh else 0.0)\n",
    "        elif not gt_present and not pred_present:\n",
    "            continue  # true negative\n",
    "        else:\n",
    "            aps.append(0.0)  # either FN or FP\n",
    "    return sum(aps) / len(aps) if aps else 0.0\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ec183c1f",
   "metadata": {},
   "source": [
    "### Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "id": "eaada1ac",
   "metadata": {},
   "source": [
    "def train_epoch(\n",
    "    model: FetusAnatomyBBRegressor,\n",
    "    train_dataloader: DataLoader,\n",
    "    optimizer: torch.optim.Optimizer # type: ignore\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Trains the model for one epoch.\n",
    "    Args:\n",
    "        model: The neural network model to train.\n",
    "        dataloader: DataLoader providing training data batches.\n",
    "        optimizer: Optimizer for updating model parameters.\n",
    "    Returns:\n",
    "        Average training loss for the epoch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    batch_tqdm = tqdm(train_dataloader, desc=\"Training batches\", leave=False, position=2)\n",
    "    for batch in batch_tqdm:\n",
    "        imgs: torch.Tensor = batch[0]\n",
    "        cls_mask: torch.Tensor = batch[1]\n",
    "        box_gt: torch.Tensor = batch[2]\n",
    "        imgs = imgs.to(DEVICE)\n",
    "        cls_mask = cls_mask.to(DEVICE)\n",
    "        box_gt = box_gt.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        cls_pred, box_pred = model(imgs)\n",
    "        loss = detection_loss(cls_pred, box_pred, cls_mask, box_gt)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    batch_tqdm.close()\n",
    "    return total_loss / len(train_dataloader)\n",
    "\n",
    "def validate(\n",
    "    model: FetusAnatomyBBRegressor,\n",
    "    val_loader: DataLoader,\n",
    "    epoch: Optional[int] = None\n",
    ") -> tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Evaluates the model on the validation set.\n",
    "    Args:\n",
    "        model: The neural network model to evaluate.\n",
    "        val_loader: DataLoader providing validation data batches.\n",
    "        epoch: Current epoch number (for logging).\n",
    "    Returns:\n",
    "        Tuple of (mean validation loss, mean mAP@0.5)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_map = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            imgs: torch.Tensor = batch[0]\n",
    "            class_mask: torch.Tensor = batch[1]\n",
    "            box_gt: torch.Tensor = batch[2]\n",
    "            imgs = imgs.to(DEVICE)\n",
    "            class_mask = class_mask.to(DEVICE)\n",
    "            box_gt = box_gt.to(DEVICE)\n",
    "            cls_pred, box_pred = model(imgs)\n",
    "            loss = detection_loss(cls_pred, box_pred, class_mask, box_gt)\n",
    "            total_loss += loss.item()\n",
    "            for i in range(imgs.size(0)):\n",
    "                ap = compute_map_per_image(\n",
    "                    class_mask[i].cpu(), box_gt[i].cpu(),\n",
    "                    cls_pred[i].cpu(), box_pred[i].cpu()\n",
    "                )\n",
    "                total_map += ap\n",
    "    mean_loss = total_loss / len(val_loader)\n",
    "    mean_map = total_map / len(val_loader.dataset) # type: ignore\n",
    "    if epoch is not None:\n",
    "        TB_WRITER.add_scalar(\"val/loss\", mean_loss, epoch)\n",
    "        TB_WRITER.add_scalar(\"val/mAP@0.5\", mean_map, epoch)\n",
    "    return mean_loss, mean_map"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "905cbd40",
   "metadata": {},
   "source": [
    "def train_one_fold(\n",
    "    model_constructor: type[FetusAnatomyBBRegressor],\n",
    "    train_subset: AnatomyDatasetSubset,\n",
    "    val_subset: AnatomyDatasetSubset,\n",
    "    checkpoint_path: str,\n",
    "    fold_idx: int,\n",
    "    n_epochs: int,\n",
    "    batch_size: int,\n",
    "    learning_rate: float,\n",
    "    weight_decay: float,\n",
    "    early_stopping_patience: int\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Trains the model on one fold of the dataset.\n",
    "    Args:\n",
    "        model_constructor: Constructor for the model to train.\n",
    "        train_subset: Training subset of the dataset.\n",
    "        val_subset: Validation subset of the dataset.\n",
    "        checkpoint_path: Path to save the best model checkpoint.\n",
    "        fold_idx: Index of the current fold (for logging).\n",
    "        n_epochs: Number of epochs to train.\n",
    "        batch_size: Batch size for training.\n",
    "        learning_rate: Learning rate for the optimizer.\n",
    "        weight_decay: Weight decay (L2 regularization) for the optimizer.\n",
    "        early_stopping_patience: Number of epochs with no improvement to wait before stopping.\n",
    "    Returns:\n",
    "        Best validation mAP@0.5 achieved during training.\n",
    "    \"\"\"\n",
    "    # Start MLflow run for the entire training process\n",
    "    # MLFlow run setup\n",
    "    mlflow_run_id = None\n",
    "    active_run = mlflow.active_run()\n",
    "    if active_run is not None:\n",
    "        mlflow_run_id = active_run.info.run_id\n",
    "    if mlflow_run_id is not None:\n",
    "        mlflow.log_params({\n",
    "            \"num_epochs\": n_epochs,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"weight_decay\": weight_decay,\n",
    "            \"model_type\": \"FetusAnatomyDetector\",\n",
    "            \"backbone\": \"MobileNetV2\"\n",
    "        }, run_id=mlflow_run_id)\n",
    "    # Initialization\n",
    "    model = model_constructor().to(DEVICE)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay) # type: ignore\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, factor=0.5)\n",
    "    train_dataloader = DataLoader(train_subset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "    val_dataloader = DataLoader(val_subset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "    best_val_map = 0.0\n",
    "    patience_counter = 0\n",
    "    epoch_tqdm = tqdm(range(n_epochs), desc=f\"Fold {fold_idx + 1} - Epochs\", position=1, leave=True)\n",
    "    for epoch in epoch_tqdm:\n",
    "        # Training\n",
    "        train_loss = train_epoch(model, train_dataloader, optimizer)\n",
    "        # Validation\n",
    "        val_loss, val_map = validate(model, val_dataloader, epoch)\n",
    "        # Get current learning rate\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        # Log to TensorBoard\n",
    "        TB_WRITER.add_scalar(\"train/loss\", train_loss, epoch)\n",
    "        TB_WRITER.add_scalar(\"train/learning_rate\", current_lr, epoch)\n",
    "        # Update learning rate scheduler\n",
    "        scheduler.step(val_loss)\n",
    "        # Log metrics to MLflow\n",
    "        if mlflow_run_id is not None:\n",
    "            mlflow.log_metrics({\n",
    "                \"train_loss\": train_loss,\n",
    "                \"val_loss\": val_loss,\n",
    "                \"val_map\": val_map,\n",
    "                \"learning_rate\": current_lr\n",
    "            }, step=epoch, run_id=mlflow_run_id)\n",
    "        # Save checkpoint if we have a better model\n",
    "        if val_map > best_val_map:\n",
    "            best_val_map = val_map\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        # Early stopping check\n",
    "        epoch_description = \\\n",
    "            f\"Fold {fold_idx + 1} - \"\\\n",
    "            f\"Train Loss: {train_loss:.2f} - \" \\\n",
    "            f\"Val Loss: {val_loss:.2f} - \"\\\n",
    "            f\"Val MAP: {val_map:.2f} - \"\\\n",
    "            f\"LR: {current_lr} - Epochs\"\n",
    "        epoch_tqdm.set_description(epoch_description)\n",
    "        if patience_counter >= early_stopping_patience:\n",
    "            break\n",
    "        # Re-augment training data for next epoch\n",
    "        train_subset.transform_data()\n",
    "    if mlflow_run_id is not None:\n",
    "        mlflow.log_metrics({\n",
    "            \"final_val_loss\": val_loss,\n",
    "            \"final_val_map\": val_map,\n",
    "            \"best_val_map\": best_val_map\n",
    "        }, run_id=mlflow_run_id)\n",
    "        dummy_input = torch.randn(1, 1, TARGET_HEIGHT, TARGET_WIDTH)\n",
    "        export_path = checkpoint_path.replace(\".pth\", \".onnx\")\n",
    "        saved_model = model_constructor()\n",
    "        saved_model.load_state_dict(torch.load(checkpoint_path))\n",
    "        torch.onnx.export(\n",
    "            saved_model.cpu(),\n",
    "            dummy_input,\n",
    "            export_path,\n",
    "            input_names=['input'],\n",
    "            output_names=['output'],\n",
    "            opset_version=11,\n",
    "            dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}\n",
    "        )\n",
    "        mlflow.log_artifact(export_path, run_id=mlflow_run_id)\n",
    "    return best_val_map"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3a97f0c3",
   "metadata": {},
   "source": [
    "class dummy_context:\n",
    "    def __init__(self, *args, **kwargs):  # Accept any arguments\n",
    "        pass\n",
    "    def __enter__(self):\n",
    "        pass\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        pass\n",
    "\n",
    "def train(\n",
    "    model_constructor: type[FetusAnatomyBBRegressor],\n",
    "    dataset: AnatomyDataset,\n",
    "    train_split: float,\n",
    "    val_split: float,\n",
    "    log_runs_to_mlflow: bool,\n",
    "    n_epochs: int,\n",
    "    batch_size: int,\n",
    "    k_folds: int,\n",
    "    learning_rate,\n",
    "    weight_decay,\n",
    "    early_stopping_patience: int\n",
    ") -> tuple[str, Optional[mlflow.ActiveRun]]:\n",
    "    \"\"\"\n",
    "    Train the model using the specified parameters.\n",
    "    \"\"\"\n",
    "    if log_runs_to_mlflow:\n",
    "        context = mlflow.start_run\n",
    "    else:\n",
    "        context = dummy_context\n",
    "    base_run_name = f\"{MLFLOW_USER}_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    n_images = len(dataset)\n",
    "    train_start, train_end = 0, int(train_split * n_images)\n",
    "    val_start, val_end = train_end, int((train_split + val_split) * n_images) if (train_split + val_split) < 1.0 else n_images\n",
    "    check_points_paths = []\n",
    "    runs = []\n",
    "    validation_maps = []\n",
    "    folds_tqdm = tqdm(range(k_folds), desc=\"Folds\", leave=True, position=0)\n",
    "    for fold in folds_tqdm:\n",
    "        run_name = f\"{base_run_name}_fold_{fold + 1}\"\n",
    "        with context(run_name=run_name) as run:\n",
    "            runs.append(run)\n",
    "            checkpoint_path=os.path.join(CHECKPOINTS_DIR, f\"{run_name}.pth\")\n",
    "            check_points_paths.append(checkpoint_path)\n",
    "            shuffled_indices = np.random.permutation(np.arange(n_images))\n",
    "            train_indices = shuffled_indices[train_start:train_end]\n",
    "            val_indices = shuffled_indices[val_start:val_end]\n",
    "            train_subset = AnatomyDatasetSubset(\n",
    "                base_dataset=dataset,\n",
    "                indices=train_indices,\n",
    "                transform=TRAIN_TRANSFORMS\n",
    "            )\n",
    "            val_subset = AnatomyDatasetSubset(\n",
    "                base_dataset=dataset,\n",
    "                indices=val_indices,\n",
    "                transform=None\n",
    "            )\n",
    "            if run_name is not None:\n",
    "                mlflow.log_input(\n",
    "                    ImageListDataset(\n",
    "                        names=train_subset.img_names,\n",
    "                        version=DATASET_VERSION,\n",
    "                        source=VERSIONING_FILE_NAME\n",
    "                    ),\n",
    "                    context=\"Train Set\"\n",
    "                )\n",
    "                mlflow.log_input(\n",
    "                    ImageListDataset(\n",
    "                        names=val_subset.img_names,\n",
    "                        version=DATASET_VERSION,\n",
    "                        source=VERSIONING_FILE_NAME\n",
    "                    ),\n",
    "                    context=\"Validation Set\"\n",
    "                )\n",
    "            val_map = train_one_fold(\n",
    "                model_constructor=model_constructor,\n",
    "                train_subset=train_subset,\n",
    "                val_subset=val_subset,\n",
    "                checkpoint_path=checkpoint_path,\n",
    "                fold_idx = fold,\n",
    "                n_epochs=n_epochs,\n",
    "                batch_size=batch_size,\n",
    "                learning_rate=learning_rate,\n",
    "                weight_decay=weight_decay,\n",
    "                early_stopping_patience=early_stopping_patience\n",
    "            )\n",
    "            validation_maps.append(val_map)\n",
    "            best_fold = np.argmax(validation_maps)\n",
    "            folds_tqdm.set_description(f\"Best fold: {best_fold + 1}, with mAP@0.5: {validation_maps[best_fold]:.3f} - Folds\")\n",
    "    best_fold = np.argmax(validation_maps)\n",
    "    return check_points_paths[best_fold], runs[best_fold]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b09d7d6e",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "id": "3f9fcdf5",
   "metadata": {},
   "source": [
    "best_model_checkpoint_path, best_run = train(\n",
    "    model_constructor=FetusAnatomyBBRegressor,\n",
    "    dataset=TRAIN_VAL_DATASET,\n",
    "    log_runs_to_mlflow=True,\n",
    "    n_epochs=NUM_EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    k_folds=K_FOLDS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    early_stopping_patience=EARLY_STOPPING_PATIENCE,\n",
    "    train_split=TRAIN_SPLIT,\n",
    "    val_split=VAL_SPLIT\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0a3d65d5",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "id": "4632356a",
   "metadata": {},
   "source": [
    "best_model = FetusAnatomyBBRegressor().to(DEVICE)\n",
    "best_model.load_state_dict(torch.load(best_model_checkpoint_path))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "38410472",
   "metadata": {},
   "source": [
    "def draw_box(\n",
    "    ax: plt.Axes, # type: ignore\n",
    "    cx: float,\n",
    "    cy: float,\n",
    "    w: float,\n",
    "    h: float,\n",
    "    img_w : int,\n",
    "    img_h: int,\n",
    "    label: str,\n",
    "    color: str,\n",
    "    style='solid'\n",
    "):\n",
    "    cx_px = cx * img_w\n",
    "    cy_px = cy * img_h\n",
    "    w_px = w * img_w\n",
    "    h_px = h * img_h\n",
    "    x = cx_px - w_px / 2\n",
    "    y = cy_px - h_px / 2\n",
    "    rect = patches.Rectangle((x, y), w_px, h_px,\n",
    "                             linewidth=2,\n",
    "                             edgecolor=color,\n",
    "                             facecolor='none',\n",
    "                             linestyle=style)\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(x, y - 5, label, color=color, fontsize=8)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b0192cab",
   "metadata": {},
   "source": [
    "@torch.no_grad()\n",
    "def visualize_prediction(\n",
    "    model: FetusAnatomyBBRegressor,\n",
    "    dataset: AnatomyDataset,\n",
    "    idx: int,\n",
    "    threshold: float = 0.3\n",
    "):\n",
    "    model.eval()\n",
    "    # Load and prepare input\n",
    "    img, class_mask, boxes_gt = dataset[idx]\n",
    "    boxes_gt = boxes_gt.numpy()\n",
    "    img_input = img.unsqueeze(0).to(DEVICE)     # [1, 1, H, W]\n",
    "    img_name = dataset.img_names[idx]\n",
    "    output = model(img_input)\n",
    "    pred_class: torch.Tensor = output[0] \n",
    "    pred_boxes: torch.Tensor = output[1]\n",
    "    pred_class = pred_class.squeeze(0).cpu()    # [K]\n",
    "    pred_boxes = pred_boxes.squeeze(0).cpu()    # [K, 4]\n",
    "    # Visualization\n",
    "    img_np = img.squeeze().numpy()              # [H, W]\n",
    "    fig, ax = plt.subplots(1, figsize=(10, 10), num=img_name)\n",
    "    ax.imshow(img_np, cmap='gray')\n",
    "    # Plot ground truth boxes (solid lines)\n",
    "    gt_classes = torch.where(class_mask == 1)[0].numpy()\n",
    "    for class_idx in gt_classes:\n",
    "        structure_name = ID_TO_STRUCTURE[class_idx]\n",
    "        cx, cy, w, h = boxes_gt[class_idx]\n",
    "        draw_box(ax, cx, cy, w, h, TARGET_WIDTH, TARGET_HEIGHT,\n",
    "                 label=f\"{structure_name} (GT)\", color=COLORS_MAP[structure_name], style='solid') # type: ignore\n",
    "    # Plot predicted boxes (dashed lines, only if prob > threshold)\n",
    "    pred_classes = np.where(pred_class.numpy() > threshold)[0]\n",
    "    pred_numpy_boxes = pred_boxes.numpy()\n",
    "    for class_idx in pred_classes:\n",
    "        structure_name = ID_TO_STRUCTURE[int(class_idx)]\n",
    "        cx, cy, w, h = pred_numpy_boxes[class_idx]\n",
    "        prob = pred_class[class_idx].item()\n",
    "        draw_box(ax, cx, cy, w, h, TARGET_WIDTH, TARGET_HEIGHT,\n",
    "                 label=f\"{structure_name} ({prob:.2f})\", color=COLORS_MAP[structure_name], style='dashed') # type: ignore\n",
    "    ax.set_title(f'Image: {img_name}')\n",
    "    ax.axis('off')\n",
    "    fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "978e9b32",
   "metadata": {},
   "source": [
    "idx = np.random.choice(len(TEST_DATASET))\n",
    "visualize_prediction(best_model, TEST_DATASET, idx)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "480f4e47",
   "metadata": {},
   "source": [
    "test_dl = DataLoader(TEST_DATASET, BATCH_SIZE, False, num_workers=0, pin_memory=True)\n",
    "mean_test_loss, mean_test_ap = validate(best_model, test_dl)\n",
    "print(f\"Mean Test Loss: {mean_test_loss:.2f}\")\n",
    "print(f\"Mean Test aAP@0.5: {mean_test_ap:.2f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "934025ee",
   "metadata": {},
   "source": [
    "## Model Registration"
   ]
  },
  {
   "cell_type": "code",
   "id": "619a5b97",
   "metadata": {},
   "source": [
    "# if best_run is not None:\n",
    "#     artifact_name = best_model_checkpoint_path.replace(\".pth\", \".onnx\").split(os.sep)[-1]\n",
    "#     model_version = mlflow.register_model(\n",
    "#         model_uri=f\"{MLFLOW_EXPERIMENT.artifact_location}/{best_run.info.run_id}/artifacts/{artifact_name}\",\n",
    "#         name=MODEL_NAME\n",
    "#     )"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "model-training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
